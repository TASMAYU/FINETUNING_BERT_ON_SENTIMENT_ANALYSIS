# -*- coding: utf-8 -*-
"""FINETUNING_BERT_ON_SENTIMENT_ANALYSIS ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fh7VieBhrU43MsC6bX-5mzwKnUNaD9sl
"""

!pip install unsloth transformers scikit-learn accelerate datasets

import pandas as pd

from datasets import load_dataset
dataset= load_dataset("imdb")

train_ds= pd.DataFrame(dataset["train"])
test_ds= pd.DataFrame(dataset["test"])

"""# Baseline model to check and compare both LG and our finetuned Bert model performance on this imdb dataset for our classification task"""

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

X_train= train_ds["text"]
y_train= train_ds["label"]
X_test= test_ds["text"]
y_test= test_ds["label"]

tfidf = TfidfVectorizer()
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf  = tfidf.transform(X_test)

lg= LogisticRegression()
lg.fit(X_train_tfidf,y_train)

y_prediction=lg.predict(X_test_tfidf)

report= classification_report(y_test,y_prediction)

print(report)

"""Finetuning Distillbert With Lora Technique (Parameter Efficient Finetuning)"""

# Exploring Qlora and lora finetuning methods more as well for parameter efficient finetuning
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from peft import get_peft_model, TaskType, LoraConfig

import numpy as np
from sklearn.metrics import accuracy_score, f1_score

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

"""will create a tokenizer function so that it will take the dataset from HF in batches to tokenize it and then feed to the model"""

def tokenize(batch):
  return(tokenizer(batch["text"],padding= True, max_length=256,truncation=True))

model= AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",num_labels=2
)

tokenized_data = dataset.map(tokenize, batched=True)

for name, module in model.named_modules():  # for checking the target modules in the models where lora adapter to be feed
    print(name)

config = LoraConfig(task_type=TaskType.SEQ_CLS,r=8,lora_alpha=32,lora_dropout=0.04,target_modules=['q_lin','k_lin,v_lin', 'o_lin'])

peft_model=get_peft_model(model,config)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_score": f1_score(labels, preds)
    }

training_arguments= TrainingArguments(
    eval_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=2e-5,
    num_train_epochs=1,
    weight_decay=0.01,
)

"""Finetuning bert (Training)"""

trainer = Trainer(
    model=peft_model,
    args=training_arguments,
    train_dataset=tokenized_data["train"].shuffle(seed=20),  # subset for Colab speed
    eval_dataset=tokenized_data["test"].shuffle(seed=20),
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

# set epoch 1 because it will take a lot of time
output = trainer.evaluate()
print(output)

"""MODEL INFERENCE"""

from transformers import pipeline
sentiment_pipeline_bert= pipeline("sentiment-analysis", model= peft_model, tokenizer=tokenizer)

# label 0 for negative and label 1 for positive
print(sentiment_pipeline_bert("I loved the movie.It was Fantastic"))
print(sentiment_pipeline_bert("The movie was a disaster. i hope i will not see a movie like this in the future"))

